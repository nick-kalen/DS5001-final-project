{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f550d8-2645-47ac-a7be-cb8e8c5e54f1",
   "metadata": {},
   "source": [
    "# Final Report\n",
    "\n",
    "DS 5001 Spring 2023 Final Project\n",
    "\n",
    "Rachel Grace Treene\n",
    "\n",
    "rg5xm@virginia.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c968a0c-33bf-4eb9-910e-5058776679da",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910bb88-5802-4fd6-87e1-f70ae5aff2b6",
   "metadata": {},
   "source": [
    "The corpus used for this project is the seven Harry Potter books written by J.K. Rowling. It has 1,118,267 tokens and 23,096 unique terms. The books in the corpus tell the story of Harry Potter and his adventures, culminating in an epic battle between himself and Voldemort, the evil wizard.\n",
    "\n",
    "In this project, I consider what Quidditch, the main sport played by people in the wizarding world, can tell us about the structure and similarity of the books to one another. My question is two-fold: first, what can we learn about the correlation between the books in the Harry Potter series? Second, what can we learn by comparing these observations with the prevalence and inclusion of Quidditch in the books?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5746d5-0845-4517-86fb-dec502c8c8ec",
   "metadata": {},
   "source": [
    "## Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0153e7-60ef-494a-9740-6e0b4ee5b79a",
   "metadata": {},
   "source": [
    "The data comes from text files, obtained from a GitHub repository which used the Harry Potter corpus for an NLP project.\n",
    "\n",
    "### Provenance\n",
    "\n",
    "The original text files are located at the following repository: https://github.com/ErikaJacobs/Harry-Potter-Text-Mining. A few errors were located after downloading which were fixed to ensure more accurate work could be done.\n",
    "\n",
    "### Location\n",
    "\n",
    "The slightly edited source files for this project have been added to the current GitHub repository and can be found in the data directory. The link to that directory is as follows: https://github.com/rachelgracetreene/text-analytics-final-project/tree/main/data.\n",
    "\n",
    "### Description\n",
    "\n",
    "The subject matter of the corpus is the fictional accounts of Harry Potter, a boy who is a wizard in England in the 1990s. The source files are structured in lines, where a line is an observation, and the average document length in terms of lines is 15744.57 lines. The average document length in terms of tokens is 189055.14 tokens.\n",
    "\n",
    "### Format\n",
    "\n",
    "The source files are TXT files. The data in the files is plaintext.\n",
    "\n",
    "### Data Model\n",
    "\n",
    "We processed and annotated the corpus and converted it into the standard text analytic data model (F2) format. The main challenge during the processing was defining the correct regular expression to select chapters; this was made difficult since the chapter titles were in all caps, but some text within chapters was also capitalized. We defined a method to chunk the books by chapter. Our OHCO included book number, chapter number, paragraph number, sentence number, and token number. Processing, annotating, and analyzing the corpus produced the following tables, with the following features:\n",
    "\n",
    "#### Output: directory of files produced to meet requirements of the project\n",
    "- **CORPUS** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/CORPUS.csv)\n",
    "    - pos_tuple: a tuple representing the part of speech and the token string\n",
    "    - pos: abbreviation representing the part of speech\n",
    "    - token_str: string representing the token with its formatting (capital letters, etc.)\n",
    "    - term_str: string representing the term without formatting like capital letters\n",
    "- **LDA-PHI** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/LDA-PHI.csv)\n",
    "    - T00, T01, T02, ... T18, T19: features representing topics 1-20 produced in LDA\n",
    "- **LDA-THETA** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/LDA-THETA.csv)\n",
    "    - T00, T01, T02, ... T18, T19: features representing topics 1-20 produced in LDA\n",
    "- **LIB** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/LIB.csv)\n",
    "    - title: title of each document\n",
    "    - chapter_regex: regex corresponding with the chapter titles\n",
    "    - book_len: number of tokens\n",
    "    - n_chaps: number of chapters\n",
    "    - kendall_sum: kendall statistic for rank correlation measurement\n",
    "- **PCA-DCM** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/PCA-DCM.csv)\n",
    "    - PC0, PC1, ... PC8, PC9: features representing principal components 1-10 produced in PCA\n",
    "    - title: title of each document\n",
    "    - chapter_regex: regex corresponding with the chapter titles\n",
    "    - book_len: number of tokens\n",
    "    - n_chaps: number of chapters\n",
    "    - kendall_sum: kendall statistic for rank correlation measurement\n",
    "    - doc: title of each document with corresponding chapter numbers of each observation\n",
    "- **PCA-LOADINGS** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/PCA-LOADINGS.csv)\n",
    "    - PC0, PC1, ... PC8, PC9: features representing principal components 1-10 produced in PCA\n",
    "- **SA-DOCEMOTIONS** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/SA-DOCEMOTIONS.csv)\n",
    "    - anger: measurement of anger sentiment in each book\n",
    "    - anticipation: measurement of anticipation sentiment in each book\n",
    "    - disgust: measurement of disgust sentiment in each book\n",
    "    - fear: measurement of fear sentiment in each book\n",
    "    - joy: measurement of joy sentiment in each book\n",
    "    - sadness: measurement of sadness sentiment in each book\n",
    "    - surprise: measurement of surprise sentiment in each book\n",
    "    - trust: measurement of trust sentiment in each book\n",
    "    - polarity: measurement of overall sentiment in each book\n",
    "- **SA-VOCAB** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/SA-VOCAB.csv)\n",
    "    - n: count of term in corpus\n",
    "    - p: probabiliy of term occurrence in corpus\n",
    "    - i: information for each term in corpus\n",
    "    - n_chars: number of characters for each term in corpus\n",
    "    - max_pos: maximally-occuring part of speech for term\n",
    "    - n_pos: number of unique parts of speech for the term\n",
    "    - cat_pos: tag for each unique part of speech for the term\n",
    "    - stop: dummy variable indicating whether a term is a stopword\n",
    "    - stem_porter: stem for the term according to the porter method\n",
    "    - stem_snowball: stem for the term according to the snowball method\n",
    "    - stem_lancaster: stem for the term according to the lancaster method\n",
    "    - dfidf: global boolean term entropy\n",
    "    - mean_tfidf: average significance of the term in a document\n",
    "    - anger, anticipation, disgust, fear, joy, sadness, surprise, trust, sentiment, negative, positive: features representing the sentiment analysis lexicon mapped to corpus terms\n",
    "    - polarity: measurement of overall sentiment for each term\n",
    "- **VOCAB** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/VOCAB.csv)\n",
    "    - n: count of term in corpus\n",
    "    - p: probabiliy of term occurrence in corpus\n",
    "    - i: information for each term in corpus\n",
    "    - n_chars: number of characters for each term in corpus\n",
    "    - max_pos: maximally-occuring part of speech for term\n",
    "    - n_pos: number of unique parts of speech for the term\n",
    "    - cat_pos: tag for each unique part of speech for the term\n",
    "    - stop: dummy variable indicating whether a term is a stopword\n",
    "    - stem_porter: stem for the term according to the porter method\n",
    "    - stem_snowball: stem for the term according to the snowball method\n",
    "    - stem_lancaster: stem for the term according to the lancaster method\n",
    "    - dfidf: global boolean term entropy\n",
    "    - mean_tfidf: average significance of the term in a document\n",
    "- **W2V-VOCAB** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output/W2V-VOCAB.csv)\n",
    "    - n: count of term in corpus\n",
    "    - p: probabiliy of term occurrence in corpus\n",
    "    - i: information for each term in corpus\n",
    "    - n_chars: number of characters for each term in corpus\n",
    "    - max_pos: maximally-occuring part of speech for term\n",
    "    - n_pos: number of unique parts of speech for the term\n",
    "    - cat_pos: tag for each unique part of speech for the term\n",
    "    - stop: dummy variable indicating whether a term is a stopword\n",
    "    - stem_porter: stem for the term according to the porter method\n",
    "    - stem_snowball: stem for the term according to the snowball method\n",
    "    - stem_lancaster: stem for the term according to the lancaster method\n",
    "    - dfidf: global boolean term entropy\n",
    "    - mean_tfidf: average significance of the term in a document\n",
    "    - vector: representation of vectorized term in (x-coord, y-coord) form\n",
    "    - x: x-coordinate for vectorized term\n",
    "    - y: y-coordinate for vectorized term\n",
    "#### Output-Viz: directory of files produced to be imported in the visualization notebook\n",
    "- **DOCS** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output-viz/DOCS.csv)\n",
    "    - n: number of tokens in the chapter\n",
    "    - book_chap_sig: significance of the chapter\n",
    "- **MT** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output-viz/MT.csv)\n",
    "    - pressed, recognized, perfectly...: aggregate tfidf (significance of term in book) for top 1000 terms of corpus by DFIDF excluding proper nouns\n",
    "- **PAIRS** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output-viz/PAIRS.csv)\n",
    "    - correl: correlation between documents a and b\n",
    "    - euclidean: euclidean distance between documents a and b\n",
    "    - cosine: cosine distance between documents a and b\n",
    "    - cityblock: cityblock distance between documents a and b\n",
    "    - jaccard: jaccard distance between documents a and b\n",
    "    - js: js distance between documents a and b\n",
    "- **POS-GROUP** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output-viz/POS-GROUP.csv)\n",
    "    - \n",
    "- **POS** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output-viz/POS.csv)\n",
    "- **SA-CHAPEMOTIONS** (https://github.com/rachelgracetreene/text-analytics-final-project/blob/main/output-viz/SA-CHAPEMOTIONS.csv)\n",
    "\n",
    "All of the methods used to perform processing, annotating the VOCAB table, LDA, Word2Vec, Sentiment Analysis, and Semantic Search are in a custom Python package called HarryPotterETA ().\n",
    "Describe the analytical tables you generated in the process of tokenization, annotation, and analysis of your corpus. You provide a list of tables with field names and their definition, along with URLs to each associated CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d35b9-8b44-472d-b036-929a5913ff8e",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f2626-134f-4a83-8140-0b12dbb6b66d",
   "metadata": {},
   "source": [
    "Describe each of your explorations, such as PCA and topic models. For each, include the relevant parameters and hyperparemeters used to generate each model and visualization. For your visualizations, you should use at least three (but likely more) of the following visualization types:\n",
    "\n",
    "    Hierarchical cluster diagrams\n",
    "    Heatmaps showing correlations - yes (correlations between books)\n",
    "    Scatter plots\n",
    "    KDE plots - yes (four houses, quidditch words)\n",
    "    Dispersion plots\n",
    "    t-SNE plots - yes (quidditch cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4974b-3ed2-4a50-9bee-d8f58bd6cf1d",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99115faa-d7c7-418c-9719-6ce83d5669c4",
   "metadata": {},
   "source": [
    "Provide your interpretation of the results of exploration, and any conclusion if you are comfortable making them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3297f-ad13-470e-b35c-c67d978fa276",
   "metadata": {},
   "source": [
    "Harry Potter and the Philosopher's Stone:\n",
    "\n",
    "    Chapter 11: \"Quidditch\"\n",
    "    Chapter 13: \"Nicolas Flamel\"\n",
    "\n",
    "Harry Potter and the Chamber of Secrets:\n",
    "\n",
    "    Chapter 10: \"The Rogue Bludger\"\n",
    "    Chapter 14: \"Cornelius Fudge\"\n",
    "\n",
    "Harry Potter and the Prisoner of Azkaban:\n",
    "\n",
    "    Chapter 9: \"Grim Defeat\"\n",
    "    Chapter 13: \"Gryffindor versus Ravenclaw\"\n",
    "\n",
    "Harry Potter and the Goblet of Fire:\n",
    "\n",
    "    Chapter 8: \"The Quidditch World Cup\"\n",
    "\n",
    "Harry Potter and the Order of Phoenix:\n",
    "\n",
    "    Chapter 19: \"The Lion and the Serpent\"\n",
    "\n",
    "Harry Potter and the Half-Blood Prince:\n",
    "\n",
    "    Chapter 14: \"Felix Felicis\"\n",
    "    Chapter 19: \"Elf Tails\"\n",
    "    Chapter 24"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
